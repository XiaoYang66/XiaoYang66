<!DOCTYPE html>
<html lang="en"><!-- Beautiful Jekyll | MIT license | Copyright Dean Attali 2016 --><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0">

  <title>Jinlan Fu</title>

  <meta name="author" content="Kai-Wei Chang">

  

  <link rel="alternate" type="application/rss+xml" title="Kai-Wei Chang - Assistant Professor at UCLA" href="http://web.cs.ucla.edu/~kwchang/feed.xml">

  <!-- <link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.2.0/css/font-awesome.min.css" rel="stylesheet"> -->
  
<link rel="stylesheet" href="https://cdn.staticfile.org/font-awesome/4.7.0/css/font-awesome.css">

      <!-- <link rel="stylesheet" href="./JinlanFu/font-awesome.min.css"> -->
    
  

  
    
      <link rel="stylesheet" href="./JinlanFu/bootstrap.min.css">
    
      <link rel="stylesheet" href="./JinlanFu/bootstrap-social.css">
    
      <link rel="stylesheet" href="./JinlanFu/main.css">
    
  

  
    
      <link rel="stylesheet" href="./JinlanFu/css">
    
      <link rel="stylesheet" href="./JinlanFu/css(1)">
    
  

  

  

  

    <!-- Facebook OpenGraph tags -->
  

  
  <!-- <meta property="og:title" content="Kai-Wei Chang"> -->
  

   
  <!-- <meta property="og:description" content="![image-title-here](/~kwchang/img/myphoto.jpg){:class=&quot;avatar-img&quot;} Kai-Wei Chang Assistant Professor @ UCLA-CS kw AT kwchang DOT net UCLA Eng VI Rm 374 Natural Language Processing Machine Learning Google Scholar Youtube Channel Twitter Curriculum Vitae News ### Announcements - Join us! Perspective students please read [this](application/). ### Upcoming Travel {% for t in site.data.news[0].news %} {%..."> -->
  


  <!-- <meta property="og:type" content="website"> -->

  
  <!-- <meta property="og:url" content="http://web.cs.ucla.edu/~kwchang/"> -->
  <!-- <link rel="canonical" href="http://web.cs.ucla.edu/~kwchang/"> -->
  

  
  <!-- <meta property="og:image" content="http://web.cs.ucla.edu/~kwchang/img/uclanlp_new.png"> -->
  
  

  <!-- Twitter summary cards -->
<!--   <meta name="twitter:card" content="summary">
  <meta name="twitter:site" content="@kaiwei_chang">
  <meta name="twitter:creator" content="@kaiwei_chang"> -->

  
  <!-- <meta name="twitter:title" content="Kai-Wei Chang"> -->
  

  
  <!-- <meta name="twitter:description" content="![image-title-here](/~kwchang/img/myphoto.jpg){:class=&quot;avatar-img&quot;} Kai-Wei Chang Assistant Professor @ UCLA-CS kw AT kwchang DOT net UCLA Eng VI Rm 374 Natural Language Processing Machine Learning Google Scholar Youtube Channel Twitter Curriculum Vitae News ### Announcements - Join us! Perspective students please read [this](application/). ### Upcoming Travel {% for t in site.data.news[0].news %} {%..."> -->
  

  
  <!-- <meta name="twitter:image" content="/~kwchang/img/uclanlp_new.png"> -->
  

<!-- <script charset="utf-8" src="./JinlanFu/button.0d6aa7fd095b2a9dd19cc66c7c2ed64b.js"></script><script charset="utf-8" src="./JinlanFu/moment_timeline.bda7aacfecfa6a7bd7d77f5f5f6c2cbb.js"></script><script charset="utf-8" src="./JinlanFu/timeline.55167c7072ca7f4363bf18820295ba93.js"></script></head> -->


  <body data-new-gr-c-s-check-loaded="14.1024.0" data-gr-ext-installed="">
  
    <nav class="navbar navbar-default navbar-fixed-top navbar-custom">
  <div class="container-fluid">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#main-navbar">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
<!--       remove navbar-brand to remove name in order to put photo on left
 --><!--       <a class="navbar-brand" href="http://web.cs.ucla.edu">Kai-Wei Chang</a>
 -->    </div>

    <div class="collapse navbar-collapse" id="main-navbar">
      <ul class="nav navbar-nav navbar-right">
      
        
<!--           <li>
<a href="http://web.cs.ucla.edu/~kwchang/">Director</a>
</li> -->
        
        
        
<!--           <li>
<a href="http://web.cs.ucla.edu/~kwchang/members">Group</a>
          </li> -->
        
        
        
<!--           <li>
<a href="http://web.cs.ucla.edu/~kwchang/publications_area">Topics</a>
          </li> -->
        
        
        
<!--           <li>
<a href="http://web.cs.ucla.edu/~kwchang/publications">Publications</a>
          </li> -->
        
        
        
<!--           <li>
<a href="http://web.cs.ucla.edu/~kwchang/talks">Talks</a>
          </li> -->
        
        
 <!--        
          <li>        
<a href="http://web.cs.ucla.edu/~kwchang/blog">News</a>
          </li>
        
        
        
          <li>
<a href="http://web.cs.ucla.edu/~kwchang/awards">Awards</a>
          </li>
        
        
        
          <li>
<a href="http://web.cs.ucla.edu/~kwchang/funding">Funding</a>
          </li>
        
        
        
          <li>
<a href="http://web.cs.ucla.edu/~kwchang/application">Opening</a>
          </li> -->
        
        
      </ul>
    </div>

	
<!-- 	<div class="avatar-container">
	  <div class="avatar-img-border">
          <a href="http://web.cs.ucla.edu/~kwchang">
	      <img class="avatar-img" src="./JinlanFu/uclanlp.png">
		</a>
	  </div>
	</div> -->
	

  </div>
</nav>


    <!-- TODO this file has become a mess, refactor it -->


<div class="intro-header"></div>

<div class="container" role="main">
  <div class="row">
    <div class="col-lg-10 col-lg-offset-1 col-md-10 col-md-offset-1">
      <div class="jumbotron profile" id="main-profile">
<div class="container">
<div class="col-md-3" align="right">
      <p><img src="./JinlanFu/jinlan-jonna.png" alt="image-title-here" class="avatar-img"></p>
    </div>
<div class="col-md-9">
<h2 align="center"> Jinlan Fu </h2>
<!-- <h2 align="center"> XXXXX </h2> -->
<div class="col-md-7 col-md-offset-1" id="main-profile">
        <ul>
  <li><a href="http://web.cs.ucla.edu/~kwchang/">
  <span class="fa-stack fa-lg">
    <i class="fa fa-circle fa-stack-2x"></i>
    <i class="fa fa-university fa-stack-1x fa-inverse"></i>
  </span>
  </a> Postdoc @ NUS-CS </li>   
  <li><a href="mailto:jinlanjonna.google.com" title="Email me">
  <span class="fa-stack fa-lg">
    <i class="fa fa-circle fa-stack-2x"></i>
    <i class="fa fa-envelope fa-stack-1x fa-inverse"></i>
  </span>
  </a> jinlanjonna AT google DOT com </li>
  <li><a href="http://web.cs.ucla.edu/~kwchang/">
  <span class="fa-stack fa-lg">
    <i class="fa fa-circle fa-stack-2x"></i>
    <i class="fa fa-location-arrow fa-stack-1x fa-inverse"></i>
  </span> </a> NUS Innovation 4.0 Rm 406</li>
  <li><a href="http://web.cs.ucla.edu/~kwchang/">
  <span class="fa-stack fa-lg">
    <i class="fa fa-circle fa-stack-2x"></i>
    <i class="fa fa-hashtag fa-stack-1x fa-inverse"></i>
  </span> </a> <span id="smallbox">Natural Language Processing<br> Machine Learning</span></li> 
  </ul>
      </div>
<div class="col-md-4" id="main-profile">
        <ul>
 <li><a href="https://scholar.google.com/citations?user=D4vtw8QAAAAJ&hl=en">
  <span class="fa-stack fa-lg">
    <i class="fa fa-circle fa-stack-2x"></i>
    <i class="fa fa-graduation-cap fa-stack-1x fa-inverse"></i>
  </span> Google Scholar </a><a href="https://scholar.google.com/citations?user=D4vtw8QAAAAJ&hl=en">
  <i class="fa fa-rss"></i>
  </a>
  </li>
  <!-- <li><a href="https://www.youtube.com/channel/UCN7nI4vDPLhnDVTXse5Vjyw">
  <span class="fa-stack fa-lg">
    <i class="fa fa-circle fa-stack-2x"></i>
    <i class="fa fa-youtube fa-stack-1x fa-inverse"></i>
  </span> Youtube Channel</a>
  </li> -->
  <li><a href="https://twitter.com/JinlanFu">
  <span class="fa-stack fa-lg">
    <i class="fa fa-circle fa-stack-2x"></i>
    <i class="fa fa-twitter fa-stack-1x fa-inverse"></i>
  </span> Twitter</a>
  </li>
 <!-- <li><a href="http://web.cs.ucla.edu/~kwchang/documents/pdf/cv.pdf">
  <span class="fa-stack fa-lg">
    <i class="fa fa-circle fa-stack-2x"></i>
    <i class="fa fa-id-badge fa-stack-1x fa-inverse"></i>
  </span> Curriculum Vitae</a>
  </li> -->

</ul>
      </div>
</div>
</div>
</div>


<div class="row">

<h2> About Me </h2>
<p>
Hi there, I am a postdoc at the National University of Singapore, working with Prof. <a class="urllink" href="https://www.comp.nus.edu.sg/~ngsk/" rel="nofollow">See-Kiong Ng</a>. 
I obtained a Ph.D. degree in the School of Computer Science at Fudan University (09.2016~06.2021). At Fudan, I am a member of the NLP group (directed by Prof. <a class="urllink" href="http://nlp.fudan.edu.cn/xjhuang" rel="nofollow"> Xuanjing Huang </a>) and the SMA lab (directed by Prof. <a class="urllink" href="http://qizhang.info/" rel="nofollow"> Qi Zhang </a>). 
<!-- where the advisor is Prof. <a class="urllink" href="http://nlp.fudan.edu.cn/xjhuang" rel="nofollow"> Xuanjing Huang </a>.  -->
I'm lucky to visit the Language Technologies Institute (LTI) of Carnegie Mellon University (2019~Now) remotely, advised by Dr. <a class="urllink" href="http://pfliu.com/" rel="nofollow"> Pengfei Liu </a> and Prof. <a class="urllink" href="http://www.phontron.com/" rel="nofollow"> Graham Neubig </a>. 
</p>
<p>
I focus on the following research perspectives for natural language: 
</p>
<ul>
<li><p> Interpretable Analysis for NLP-oriented Tasks; </p></li>
<li><p> Information Extraction, Sequence Labeling, Question Answering; </p></li>
<li><p> Cross-lingual Transfer Learning. </p></li>
<!-- <li><p> Question Answering </p></li> -->
</ul>

</div>

<hr>

<div class="row">
  <h2 id="about-me">Awards</h2>
  <ul>
  <li>
    <p><a href="https://arxiv.org/pdf/2104.06387.pdf"><b>Best Demo Award, ACL 2021 </b></a>: ExplainaBoard: An Explainable Leaderboard for NLP</p>
  </li>
  <li>
    <p>Outstanding Graduate, Fudan University, 2021</p>
  </li>
  <li>
    <p>2017-2018 and 2019-2020, National Scholarship </p>
  </li>
</ul>
</div>
<hr>


<!-- 
<div class="row">
  <h2 id="about-me">Projects</h2>
  <ul>
  <li>
    <p>ExplainaBoard: An Explainable Leaderboard for NLP: <a href="http://explainaboard.nlpedia.ai/">Homepage </a></p>
  </li>
  <li>
    <p>Outstanding Graduate, Fudan University, 2021</p>
  </li>
  <li>
    <p>2017-2018 and 2019-2020, National Scholarship </p>
  </li>
</ul>
</div>
<hr>
 -->


<div class="row">
<h2 id="about-me">Projects</h2>
<ul class="bibliography">

<li>
    <span class="project_text">ExplainaBoard: An Explainable Leaderboard for NLP: </span>
    <a href="http://explainaboard.nlpedia.ai/" class="pj_home">Homepage</a>
    <a href="https://github.com/neulab/ExplainaBoard" class="pj_home">Code</a>
</li>

<li>
    <span class="project_text">NLPedia Paper Searching System: </span>  
    <a href="http://nlpedia.pub/" class="pj_home">Homepage</a>
    <!-- <a href="None" class="pj_home">Code</a> -->
</li>

<li>
    <span class="project_text">NER Paperlist: </span>  
    <a href="http://pfliu.com/ner/ner.html" class="pj_home">Homepage</a>
    <a href="https://github.com/pfliu-nlp/Named-Entity-Recognition-NER-Papers" class="pj_home">Code</a>
</li>

</ul>
</div>

<hr>














<div class="row">
  <h2 id="about-me">Selected Publications</h2>
  <p>A complete list is in <a href="https://scholar.google.com/citations?user=D4vtw8QAAAAJ&hl=en" target="_blank">Google Scholar</a>.<br>
  </p>


<h2 class="bibliography">2021</h2>
<ul class="bibliography">

<li>
<h4> <a href="https://arxiv.org/pdf/2104.06387.pdf"> EXPLAINABOARD: An Explainable Leaderboard for NLP </a></h4>

<span id="meng2020integer">Pengfei Liu, <b>Jinlan Fu</b>, Yang Xiao, Weizhe Yuan, Shuaicheng Chang, Junqi Dai, Yixin Liu, Zihuiwen Ye, Zi-Yi Dou, Graham Neubig</span>
<br>
    <span class="conf">ACL, Best Demo </span>

    <a href="https://arxiv.org/pdf/2104.06387.pdf" class="my_details">Full Text</a>

    <a href="https://github.com/neulab/ExplainaBoard" class="my_code">Code</a>

    <a href="http://explainaboard.nlpedia.ai/" class="my_code">ExplainaBoard</a>
      
    <a data-toggle="collapse" href="#liu2021explain-abstract" class="my_details">Abstract</a>
    

<a data-toggle="collapse" href="#liu2021explain-bibtex" class="my_details">BibTeX</a>

<!-- <a href="http://web.cs.ucla.edu/~kwchang/bibliography/liu2021explain/" class="my_details">Details</a> -->
 

<div id="liu2021explain-materials">
  
  <pre id="liu2021explain-abstract" class="pre collapse">With the rapid development of NLP research, leaderboards have emerged as one tool to track the performance of various systems on various NLP tasks. They are effective in this goal to some extent, but generally present a rather simplistic one-dimensional view of the submitted systems, communicated only through holistic accuracy numbers. In this paper, we present a new conceptualization and implementation of NLP evaluation: the ExplainaBoard, which in addition to inheriting the functionality of the standard leaderboard, also allows researchers to (i) diagnose strengths and weaknesses of a single system (e.g.~what is the best-performing system bad at?) (ii) interpret relationships between multiple systems. (e.g.~where does system A outperform system B? What if we combine systems A, B, and C?) and (iii) examine prediction results closely (e.g.~what are common errors made by multiple systems, or in what contexts do particular errors occur?). So far, ExplainaBoard covers more than 400 systems, 50 datasets, 40 languages, and 12 tasks. ExplainaBoard keeps updated and is recently upgraded by supporting (1) multilingual multi-task benchmark, (2) meta-evaluation, and (3) more complicated task: machine translation, which reviewers also suggested.} We not only released an online platform on the website \url{http://explainaboard.nlpedia.ai/} but also make our evaluation tool an API with MIT Licence at Github \url{https://github.com/neulab/explainaBoard} and PyPi \url{https://pypi.org/project/interpret-eval/} that allows users to conveniently assess their models offline. We additionally release all output files from systems that we have run or collected to motivate "output-driven" research in the future. </pre>
  

  <pre id="liu2021explain-bibtex" class="pre pre-scrollable collapse">@inproceedings{liu2021explain,
  title = {EXPLAINABOARD: An Explainable Leaderboard for NLP},
  author = {Pengfei Liu, Jinlan Fu, Yang Xiao, Weizhe Yuan, Shuaicheng Chang, Junqi Dai, Yixin Liu, Zihuiwen Ye, Zi-Yi Dou, Graham Neubig},
  booktitle = {ACL},
  year = {2021}
}
</pre> 
</div>
</li>

<!-- <a class="details" href="http://web.cs.ucla.edu/bibliography/liu2021explain/">Details</a></li>
 -->

<li>
<h4> <a href="https://arxiv.org/pdf/2104.07412.pdf"> XTREME-R: Towards More Challenging and Nuanced Multilingual Evaluation </a></h4>

<span id="ruder2021xtremer">Sebastian Ruder, Noah Constant, Jan Botha, Aditya Siddhant, Orhan Firat, <b>Jinlan Fu</b>, Pengfei Liu, Junjie Hu, Graham Neubig, Melvin Johnson </span>
<br>

    <span class="conf">arXiv</span>

    <a href="https://arxiv.org/pdf/2104.07412.pdf" class="my_details">Full Text</a>
    <a href="https://github.com/google-research/xtreme" class="my_code">Code</a>
    <a href="https://sites.research.google/xtreme/" class="my_code">Leaderboard</a>
    <a href="http://explainaboard.nlpedia.ai/leaderboard/xtreme/" class="my_code">ExplainaBoard</a>

      
    <a data-toggle="collapse" href="#ruder2021xtremer-abstract" class="my_details">Abstract</a>
    
<a data-toggle="collapse" href="#ruder2021xtremer-bibtex" class="my_details">BibTeX</a>

<div id="ruder2021xtremer-materials">
  
  <pre id="ruder2021xtremer-abstract" class="pre collapse">Machine learning has brought striking advances in multilingual natural language processing capabilities over the past year. For example, the latest techniques have improved the state-of-the-art performance on the XTREME multilingual benchmark by more than 13 points. While a sizeable gap to human-level performance remains, improvements have been easier to achieve in some tasks than in others. This paper analyzes the current state of cross-lingual transfer learning and summarizes some lessons learned. In order to catalyze meaningful progress, we extend XTREME to XTREME-R, which consists of an improved set of ten natural language understanding tasks, including challenging language-agnostic retrieval tasks, and covers 50 typologically diverse languages. In addition, we provide a massively multilingual diagnostic suite and fine-grained multi-dataset evaluation capabilities through an interactive public leaderboard to gain a better understanding of such models.</pre>
  
  <pre id="ruder2021xtremer-bibtex" class="pre pre-scrollable collapse">@inproceedings{ruder2021xtremer,
  title = {XTREME-R: Towards More Challenging and Nuanced Multilingual Evaluation},
  author = {Sebastian Ruder, Noah Constant, Jan Botha, Aditya Siddhant, Orhan Firat, Jinlan Fu, Pengfei Liu, Junjie Hu, Graham Neubig, Melvin Johnson},
  booktitle = {arXiv},
  year = {2021}
}
</pre> 
</div>
</li>



<li>
<h4> <a href="https://arxiv.org/pdf/2106.00641.pdf"> SpanNER: Named Entity Re-/Recognition as Span Prediction </a></h4>

<span id="fu2021spanner"><b>Jinlan Fu</b>, Xuanjing Huang, Pengfei Liu </span>
<br>
    <span class="conf">ACL</span>
    
    <a href="https://arxiv.org/pdf/2106.00641.pdf" class="my_details">Full Text</a>

    <a href="https://github.com/neulab/spanner" class="my_code">Code</a>
    <a href="http://explainaboard.nlpedia.ai/leaderboard/task-ner/" class="my_code">Demo</a>
      
    <a data-toggle="collapse" href="#fu2021spanner-abstract" class="my_details">Abstract</a>
    

<a data-toggle="collapse" href="#fu2021spanner-bibtex" class="my_details">BibTeX</a>

    
<div id="fu2021spanner-materials">
  
  <pre id="fu2021spanner-abstract" class="pre collapse">Recent years have seen the paradigm shift of Named Entity Recognition (NER) systems from sequence labeling to span prediction. Despite its preliminary effectiveness, the span prediction model's architectural bias has not been fully understood. In this paper, we first investigate the strengths and weaknesses when the span prediction model is used for named entity recognition compared with the sequence labeling framework and how to further improve it, which motivates us to make complementary advantages of systems based on different paradigms. We then reveal that span prediction, simultaneously, can serve as a system combiner to re-recognize named entities from different systems' outputs. We experimentally implement 154 systems on 11 datasets, covering three languages, comprehensive results show the effectiveness of span prediction models that both serve as base NER systems and system combiners. We make all code and datasets available: \url{https://github.com/neulab/spanner}, as well as an online system demo: \url{http://spanner.sh}. Our model also has been deployed into the ExplainaBoard platform, which allows users to flexibly perform a system combination of top-scoring systems in an interactive way: \url{http://explainaboard.nlpedia.ai/leaderboard/task-ner/}.</pre>
  

  <pre id="fu2021spanner-bibtex" class="pre pre-scrollable collapse">@inproceedings{fu2021spanner,
  title = {SpanNER: Named Entity Re-/Recognition as Span Prediction},
  author = {Jinlan Fu, Xuanjing Huang, Pengfei Liu},
  booktitle = {ACL},
  year = {2021}
}
</pre> 
</div>
</li>



<li>
<h4> <a href="https://arxiv.org/pdf/2104.04434.pdf"> Larger-Context Tagging: When and Why Does It Work? </a></h4>

<span id="fu2021spanner"><b>Jinlan Fu</b>, Liangjing Feng, Qi Zhang, Xuanjing Huang, Pengfei Liu </span>
<br>
    <span class="conf">NAACL</span>
    
    <a href="https://arxiv.org/pdf/2104.04434.pdf" class="my_details">Full Text</a>

    <a href="http://explainaboard.nlpedia.ai/leaderboard/task-ner/" class="my_code">Demo</a>
    <a data-toggle="collapse" href="#fu2021larger-abstract" class="my_details">Abstract</a>
    

<a data-toggle="collapse" href="#fu2021larger-bibtex" class="my_details">BibTeX</a>

    
<div id="fu2021larger-materials">
  
  <pre id="fu2021larger-abstract" class="pre collapse">The development of neural networks and pretraining techniques has spawned many sentence-level tagging systems that achieved superior performance on typical benchmarks. However, a relatively less discussed topic is what if more context information is introduced into current top-scoring tagging systems. Although several existing works have attempted to shift tagging systems from sentence-level to document-level, there is still no consensus conclusion about when and why it works, which limits the applicability of the larger-context approach in tagging tasks. In this paper, instead of pursuing a state-of-the-art tagging system by architectural exploration, we focus on investigating when and why the larger-context training, as a general strategy, can work. To this end, we conduct a thorough comparative study on four proposed aggregators for context information collecting and present an attribute-aided evaluation method to interpret the improvement brought by larger-context training. Experimentally, we set up a testbed based on four tagging tasks and thirteen datasets. Hopefully, our preliminary observations can deepen the understanding of larger-context training and enlighten more follow-up works on the use of contextual information.</pre>
  

  <pre id="fu2021larger-bibtex" class="pre pre-scrollable collapse">@inproceedings{fu2021larger,
  title = {Larger-Context Tagging: When and Why Does It Work?},
  author = {Jinlan Fu, Liangjing Feng, Qi Zhang, Xuanjing Huang, Pengfei Liu},
  booktitle = {NAACL},
  year = {2021}
}
</pre> 
</div>
</li>




<li>
<h4> <a href="https://arxiv.org/pdf/2102.05486.pdf"> Towards More Fine-grained and Reliable NLP Performance Prediction </a></h4>

<span id="ye2021towards">Zihuiwen Ye, Pengfei Liu, <b>Jinlan Fu</b>, Graham Neubig </span>
<br>
    <span class="conf">EACL</span>
    
    <a href="https://arxiv.org/pdf/2102.05486.pdf" class="my_details">Full Text</a>
    <a href="https://github.com/neulab/Reliable-NLPPP" class="my_code">Code</a>
    <a href="http://explainaboard.nlpedia.ai/leaderboard/task-ner/" class="my_code">Demo</a>

    <a data-toggle="collapse" href="#ye2021towards-abstract" class="my_details">Abstract</a>
    

<a data-toggle="collapse" href="#ye2021towards-bibtex" class="my_details">BibTeX</a>

    
<div id="ye2021towards-materials">
  
  <pre id="ye2021towards-abstract" class="pre collapse">Performance prediction, the task of estimating a system's performance without performing experiments, allows us to reduce the experimental burden caused by the combinatorial explosion of different datasets, languages, tasks, and models. In this paper, we make two contributions to improving performance prediction for NLP tasks. First, we examine performance predictors not only for holistic measures of accuracy like F1 or BLEU but also fine-grained performance measures such as accuracy over individual classes of examples. Second, we propose methods to understand the reliability of a performance prediction model from two angles: confidence intervals and calibration. We perform an analysis of four types of NLP tasks, and both demonstrate the feasibility of fine-grained performance prediction and the necessity to perform reliability analysis for performance prediction methods in the future. We make our code publicly available: \url{https://github.com/neulab/Reliable-NLPPP} </pre>
  

  <pre id="ye2021towards-bibtex" class="pre pre-scrollable collapse">@inproceedings{ye2021towards,
  title = {Towards More Fine-grained and Reliable NLP Performance Prediction},
  author = {Zihuiwen Ye, Pengfei Liu, Jinlan Fu, Graham Neubig},
  booktitle = {EACL},
  year = {2021}
}
</pre> 
</div>
</li>
</ul>
<!-- </div>   -->



<h2 class="bibliography">2020</h2>
<ul class="bibliography">

<li>
<h4> <a href="https://arxiv.org/pdf/2011.06854.pdf"> Interpretable Multi-dataset Evaluation for Named Entity Recognition </a></h4>

<span id="fu2020interpret"><b>Jinlan Fu</b>, Pengfei Liu, Graham Neubig </span>
<br>
    <span class="conf">EMNLP</span>
    
    <a href="https://arxiv.org/pdf/2011.06854.pdf" class="my_details">Full Text</a>

    <a href="https://github.com/neulab/InterpretEval" class="my_code">Code</a>
    <a href="http://explainaboard.nlpedia.ai/leaderboard/task-ner/" class="my_code">Demo</a>
    
    <a data-toggle="collapse" href="#fu2020interpret-abstract" class="my_details">Abstract</a>
    

<a data-toggle="collapse" href="#fu2020interpret-bibtex" class="my_details">BibTeX</a>

    
<div id="fu2020interpret-materials">
  
  <pre id="fu2020interpret-abstract" class="pre collapse">With the proliferation of models for natural language processing tasks, it is even harder to understand the differences between models and their relative merits. Simply looking at differences between holistic metrics such as accuracy, BLEU, or F1 does not tell us why or how particular methods perform differently and how diverse datasets influence the model design choices. In this paper, we present a general methodology for interpretable evaluation for the named entity recognition (NER) task. The proposed evaluation method enables us to interpret the differences in models and datasets, as well as the interplay between them, identifying the strengths and weaknesses of current systems. By making our analysis tool available, we make it easy for future researchers to run similar analyses and drive progress in this area: https://github.com/neulab/InterpretEval. </pre>
  

  <pre id="fu2020interpret-bibtex" class="pre pre-scrollable collapse">@inproceedings{fu2020interpret,
  title = {Interpretable Multi-dataset Evaluation for Named Entity Recognition},
  author = {Jinlan Fu, Pengfei Liu, Graham Neubig},
  booktitle = {EMNLP},
  year = {2020}
}
</pre> 
</div>
</li>




<li>
<h4> <a href="https://arxiv.org/pdf/2011.06858.pdf"> RethinkCWS: Is Chinese Word Segmentation a Solved Task? </a></h4>

<span id="fu2020rethinkcws"> <b>Jinlan Fu</b>, Pengfei Liu, Qi Zhang, Xuanjing Huang </span>
<br>
    <span class="conf">EMNLP</span>
    
    <a href="https://arxiv.org/pdf/2011.06858.pdf" class="my_details">Full Text</a>
  <a href="https://github.com/neulab/InterpretEval" class="my_code">Code</a>
    <a href="http://explainaboard.nlpedia.ai/leaderboard/task-cws/" class="my_code">Demo</a>
    
    
    <a data-toggle="collapse" href="#fu2020rethinkcws-abstract" class="my_details">Abstract</a>
    

<a data-toggle="collapse" href="#fu2020rethinkcws-bibtex" class="my_details">BibTeX</a>

    
<div id="fu2020rethinkcws-materials">
  
  <pre id="fu2020rethinkcws-abstract" class="pre collapse">The performance of the Chinese Word Segmentation (CWS) systems has gradually reached a plateau with the rapid development of deep neural networks, especially the successful use of large pre-trained models. In this paper, we take stock of what we have achieved and rethink what's left in the CWS task. Methodologically, we propose a fine-grained evaluation for existing CWS systems, which not only allows us to diagnose the strengths and weaknesses of existing models (under the in-dataset setting), but enables us to quantify the discrepancy between different criterion and alleviate the negative transfer problem when doing multi-criteria learning. Strategically, despite not aiming to propose a novel model in this paper, our comprehensive experiments on eight models and seven datasets, as well as thorough analysis, could search for some promising direction for future research. We make all codes publicly available and release an interface that can quickly evaluate and diagnose user's models: https://github.com/neulab/InterpretEval.  </pre>
  

  <pre id="fu2020rethinkcws-bibtex" class="pre pre-scrollable collapse">@inproceedings{fu2020rethinkcws,
  title = {RethinkCWS: Is Chinese Word Segmentation a Solved Task?},
  author = {Jinlan Fu, Pengfei Liu, Qi Zhang, Xuanjing Huang},
  booktitle = {EMNLP},
  year = {2020}
}
</pre> 
</div>
</li>






<li>
<h4> <a href="https://arxiv.org/pdf/2001.03844.pdf"> Rethinking Generalization of Neural Models: A Named Entity Recognition Case Study </a></h4>

<span id="fu2020rethinking"> <b>Jinlan Fu</b>, Pengfei Liu, Qi Zhang, Xuanjing Huang </span>
<br>
    <span class="conf">AAAI</span>
    
    <a href="https://arxiv.org/pdf/2001.03844.pdf" class="my_details">Full Text</a>
    <a href="http://pfliu.com/InterpretNER/interpretNER.html" class="my_code">Data</a>

    
    <a data-toggle="collapse" href="#fu2020rethinking-abstract" class="my_details">Abstract</a>
    

<a data-toggle="collapse" href="#fu2020rethinking-bibtex" class="my_details">BibTeX</a>

    
<div id="fu2020rethinking-materials">
  
  <pre id="fu2020rethinking-abstract" class="pre collapse">While neural network-based models have achieved impressive performance on a large body of NLP tasks, the generalization behavior of different models remains poorly understood: Does this excellent performance imply a perfect generalization model, or are there still some limitations? In this paper, we take the NER task as a testbed to analyze the generalization behavior of existing models from different perspectives and characterize the differences of their generalization abilities through the lens of our proposed measures, which guides us to better design models and training methods. Experiments with in-depth analyses diagnose the bottleneck of existing neural NER models in terms of breakdown performance analysis, annotation errors, dataset bias, and category relationships, which suggest directions for improvement. We have released the datasets:(ReCoNLL, PLONER) for the future research at our project page: http://pfliu.com/InterpretNER/. </pre>
  

  <pre id="fu2020rethinking-bibtex" class="pre pre-scrollable collapse">@inproceedings{fu2020rethinking,
  title = {Rethinking Generalization of Neural Models: A Named Entity Recognition Case Study.},
  author = {Jinlan Fu, Pengfei Liu, Qi Zhang, Xuanjing Huang},
  booktitle = {AAAI},
  year = {2020}
}
</pre> 
</div>
</li>





<li>
<h4> <a href="https://dl.acm.org/doi/10.1145/3336191.3371817"> Recurrent Memory Reasoning Network for Expert Finding in Community Question Answering </a></h4>

<span id="fu2020recurrent"> <b>Jinlan Fu</b>, Yi Li, Qi Zhang, Qinzhuo Wu, Renfeng Ma, Xuanjing Huang, Yu-Gang Jiang </span>
<br>
    <span class="conf">WSDM</span>
    
    <a href="https://dl.acm.org/doi/10.1145/3336191.3371817" class="my_details">Full Text</a>

    
    <a data-toggle="collapse" href="#fu2020recurrent-abstract" class="my_details">Abstract</a>
    

<a data-toggle="collapse" href="#fu2020recurrent-bibtex" class="my_details">BibTeX</a>

    
<div id="fu2020recurrent-materials">
  
  <pre id="fu2020recurrent-abstract" class="pre collapse">Expert finding is a task designed to enable recommendation of the right person who can provide high-quality answers to a requester's question. Most previous works try to involve a content-based recommendation, which only superficially comprehends the relevance between a requester's question and the expertise of candidate experts by exploring the content or topic similarity between the requester's question and the candidate experts' historical answers. However, if a candidate expert has never answered a question similar to the requester's question, then existing methods have difficulty making a correct recommendation. Therefore, exploring the implicit relevance between a requester's question and a candidate expert's historical records by perception and reasoning should be taken into consideration. In this study, we propose a novel \textslrecurrent memory reasoning network (RMRN) to perform this task. This method focuses on different parts of a question, and accordingly retrieves information from the histories of the candidate expert.Since only a small percentage of historical records are relevant to any requester's question, we introduce a Gumbel-Softmax-based mechanism to select relevant historical records from candidate experts' answering histories. To evaluate the proposed method, we constructed two large-scale datasets drawn from Stack Overflow and Yahoo! Answer. Experimental results on the constructed datasets demonstrate that the proposed method could achieve better performance than existing state-of-the-art methods. </pre>
  

  <pre id="fu2020recurrent-bibtex" class="pre pre-scrollable collapse">@inproceedings{fu2020rethinkcws,
  title = {Recurrent Memory Reasoning Network for Expert Finding in Community Question Answering},
  author = {Jinlan Fu, Yi Li, Qi Zhang, Qinzhuo Wu, Renfeng Ma, Xuanjing Huang, Yu-Gang Jiang},
  booktitle = {WSDM},
  year = {2020}
}
</pre> 
</div>
</li>
</ul>







<h2 class="bibliography">2019</h2>
<ul class="bibliography">

<li>
<h4> <a href="https://arxiv.org/pdf/1906.01378.pdf"> Distantly Supervised Named Entity Recognition using Positive-Unlabeled Learning </a></h4>

<span id="peng2019distantly">Minlong Peng, Xiaoyu Xing, Qi Zhang, <b>Jinlan Fu</b>, Xuanjing Huang </span>
<br>
    <span class="conf">ACL</span>
    
    <a href="https://arxiv.org/pdf/1906.01378.pdf" class="my_details">Full Text</a>
    <a href="https://github.com/v-mipeng/LexiconNER" class="my_code">Code</a>

    <a data-toggle="collapse" href="#peng2019distantly-abstract" class="my_details">Abstract</a>
    

<a data-toggle="collapse" href="#peng2019distantly-bibtex" class="my_details">BibTeX</a>

    
<div id="peng2019distantly-materials">
  
  <pre id="peng2019distantly-abstract" class="pre collapse">In this work, we explore the way to perform named entity recognition (NER) using only unlabeled data and named entity dictionaries. To this end, we formulate the task as a positive-unlabeled (PU) learning problem and accordingly propose a novel PU learning algorithm to perform the task. We prove that the proposed algorithm can unbiasedly and consistently estimate the task loss as if there is fully labeled data. A key feature of the proposed method is that it does not require the dictionaries to label every entity within a sentence, and it even does not require the dictionaries to label all of the words constituting an entity. This greatly reduces the requirement on the quality of the dictionaries and makes our method generalize well with quite simple dictionaries. Empirical studies on four public NER datasets demonstrate the effectiveness of our proposed method. We have published the source code at \url{https://github.com/v-mipeng/LexiconNER}. </pre>
  

  <pre id="peng2019distantly-bibtex" class="pre pre-scrollable collapse">@inproceedings{peng2019distantly,
  title = {Distantly Supervised Named Entity Recognition using Positive-Unlabeled Learning},
  author = {Minlong Peng, Xiaoyu Xing, Qi Zhang, Jinlan Fu, Xuanjing Huang},
  booktitle = {ACL},
  year = {2019}
}
</pre> 
</div>
</li>




<li>
<h4> <a href="https://arxiv.org/pdf/1905.12277.pdf"> Learning Task-specific Representation for Novel Words in Sequence Labeling </a></h4>

<span id="peng2019learning">Minlong Peng, Qi Zhang, Xiaoyu Xing, Tao Gui, <b>Jinlan Fu</b>, Xuanjing Huang </span>
<br>
    <span class="conf">IJCAI</span>
    
    <a href="https://arxiv.org/pdf/1905.12277.pdf" class="my_details">Full Text</a>
    
    <a data-toggle="collapse" href="#peng2019learning-abstract" class="my_details">Abstract</a>
    

<a data-toggle="collapse" href="#peng2019learning-bibtex" class="my_details">BibTeX</a>

    
<div id="peng2019learning-materials">
  
  <pre id="peng2019learning-abstract" class="pre collapse">Word representation is a key component in neural-network-based sequence labeling systems. However, representations of unseen or rare words trained on the end task are usually poor for appreciable performance. This is commonly referred to as the out-of-vocabulary (OOV) problem. In this work, we address the OOV problem in sequence labeling using only training data of the task. To this end, we propose a novel method to predict representations for OOV words from their surface-forms (e.g., character sequence) and contexts. The method is specifically designed to avoid the error propagation problem suffered by existing approaches in the same paradigm. To evaluate its effectiveness, we performed extensive empirical studies on four part-of-speech tagging (POS) tasks and four named entity recognition (NER) tasks. Experimental results show that the proposed method can achieve better or competitive performance on the OOV problem compared with existing state-of-the-art methods. </pre>
  

  <pre id="peng2019learning-bibtex" class="pre pre-scrollable collapse">@inproceedings{peng2019learning,
  title = {Learning Task-specific Representation for Novel Words in Sequence Labeling},
  author = {Minlong Peng, Qi Zhang, Xiaoyu Xing, Tao Gui, Jinlan Fu, Xuanjing Huang},
  booktitle = {IJCAI},
  year = {2019}
}
</pre> 
</div>
</li>
</ul>






<h2 class="bibliography">2018</h2>
<ul class="bibliography">

<li>
<h4> <a href="https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/viewFile/16432/16127"> Adaptive Co-Attention Network for Named Entity Recognition in Tweets </a></h4>

<span id="zhang2018adaptive">Qi Zhang, <b>Jinlan Fu</b>, Xiaoyu Liu, Xuanjing Huang </span>
<br>
    <span class="conf">AAAI</span>
    
    <a href="https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/viewFile/16432/16127" class="my_details">Full Text</a>
    <a href="https://github.com/jlfu/NERmultimodal" class="my_code">Code</a>


    
    <a data-toggle="collapse" href="#zhang2018adaptive-abstract" class="my_details">Abstract</a>
    

<a data-toggle="collapse" href="#zhang2018adaptive-bibtex" class="my_details">BibTeX</a>

    
<div id="zhang2018adaptive-materials">
  
  <pre id="zhang2018adaptive-abstract" class="pre collapse">In this study, we investigate the problem of named entity recognition for tweets. Named entity recognition is an important task in natural language processing and has been carefully studied in recent decades. Previous named entity recognition methods usually only used the textual content when processing tweets. However, many tweets contain not only textual content, but also images. Such visual information is also valuable in the name entity recognition task. To make full use of textual and visual information, this paper proposes a novel method to process tweets that contain multimodal information. We extend a bi-directional long short term memory network with conditional random fields and an adaptive co-attention network to achieve this task. To evaluate the proposed methods, we constructed a large scale labeled dataset that contained multimodal tweets. Experimental results demonstrated that the proposed method could achieve a better performance than the previous methods in most cases. </pre>
  

  <pre id="zhang2018adaptive-bibtex" class="pre pre-scrollable collapse">@inproceedings{zhang2018adaptive,
  title = {Adaptive Co-Attention Network for Named Entity Recognition in Tweets},
  author = {Qi Zhang, Jinlan Fu, Xiaoyu Liu, Xuanjing Huang},
  booktitle = {AAAI},
  year = {2018}
}
</pre> 
</div>
</li>



<li>
<h4> <a href="file:///Users/jinlanfu/Downloads/11959-Article%20Text-15487-1-2-20201228.pdf"> Neural Networks Incorporating Dictionaries for Chinese Word Segmentation </a></h4>

<span id="zhang2018neural">Qi Zhang, Xiaoyu Liu, <b>Jinlan Fu</b> </span>
<br>
    <span class="conf">AAAI</span>
    
    <a href="file:///Users/jinlanfu/Downloads/11959-Article%20Text-15487-1-2-20201228.pdf" class="my_details">Full Text</a>


    
    <a data-toggle="collapse" href="#zhang2018neural-abstract" class="my_details">Abstract</a>
    

<a data-toggle="collapse" href="#zhang2018neural-bibtex" class="my_details">BibTeX</a>

    
<div id="zhang2018neural-materials">
  
  <pre id="zhang2018neural-abstract" class="pre collapse">In recent years, deep neural networks have achieved significant success in Chinese word segmentation and many other natural language processing tasks. Most of these algorithms are end-to-end trainable systems and can effectively process and learn from large scale labeled datasets. However, these methods typically lack the capability of processing rare words and data whose domains are different from training data. Previous statistical methods have demonstrated that human knowledge can provide valuable information for handling rare cases and domain shifting problems. In this paper, we seek to address the problem of incorporating dictionaries into neural networks for the Chinese word segmentation task. Two different methods that extend the bi-directional long short-term memory neural network are proposed to perform the task. To evaluate the performance of the proposed methods, state-of-the-art supervised models based methods and domain adaptation approaches are compared with our methods on nine datasets from different domains. The experimental results demonstrate that the proposed methods can achieve better performance than other state-of-the-art neural network methods and domain adaptation approaches in most cases. </pre>
  

  <pre id="zhang2018neural-bibtex" class="pre pre-scrollable collapse">@inproceedings{zhang2018neural,
  title = {Neural Networks Incorporating Dictionaries for Chinese Word Segmentation},
  author = {Qi Zhang, Xiaoyu Liu, Jinlan Fu},
  booktitle = {AAAI},
  year = {2018}
}
</pre> 
</div>
</li>
</ul>






</div>  
<hr>












<div class="row">
<h2> Service </h2>
<ul>
      <li>Senior PC: IJCAI 2021</li>
      <li>Conference Reviewer: ACL/EMNLP/IJCAI/AAAI (since 2019); CCL 2020.</li>
      <li>Journal Reviewer: Expert Systems with Applications</li>
    </ul>
</div>

<hr>




</div>

	    
    </div>
  </div>
</div>


    <footer>

  <div class="container beautiful-jekyll-footer">
    <div class="row">
      <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
        <ul class="list-inline text-center footer-links">
          
          
          <li>
            <a href="https://github.com/neulab" title="GitHub">
              <span class="fa-stack fa-lg">
                <i class="fa fa-circle fa-stack-2x"></i>
                <i class="fa fa-github fa-stack-1x fa-inverse"></i>
              </span>
            </a>
          </li>
          

          <li>
            <a href="https://twitter.com/JinlanFu" title="Twitter">
              <span class="fa-stack fa-lg">
                <i class="fa fa-circle fa-stack-2x"></i>
                <i class="fa fa-twitter fa-stack-1x fa-inverse"></i>
              </span>
            </a>
          </li>

          
      
		  
          <li>
            <a href="mailto:jinlanjonna@google.com" title="Email me">
              <span class="fa-stack fa-lg">
                <i class="fa fa-circle fa-stack-2x"></i>
                <i class="fa fa-envelope fa-stack-1x fa-inverse"></i>
              </span>
            </a>
          </li>
		  
      
      <!-- 
          <li>
            <a href="https://www.youtube.com/channel/UCN7nI4vDPLhnDVTXse5Vjyw" title="YouTube">
              <span class="fa-stack fa-lg">
                <i class="fa fa-circle fa-stack-2x"></i>
                <i class="fa fa-youtube fa-stack-1x fa-inverse"></i>
              </span>
            </a>
          </li> -->
          
      
		  
        </ul>
        <p class="copyright text-muted">
		  Jinlan Fu
		  &nbsp;•&nbsp;
		  2021

		  
		  <!-- &nbsp;•&nbsp; -->
		  <!-- <a href="http://web.cs.ucla.edu/">kwchang.net</a> -->
		  
	    </p>
	        <!-- Please don't remove this, keep my open source work credited :) -->
		<!-- <p class="theme-by text-muted">
		  Theme by
		  <a href="http://deanattali.com/beautiful-jekyll/">beautiful-jekyll</a>
		</p> -->
      </div>
    </div>
  </div>
</footer>

  
    






  
    <!-- doing something a bit funky here because I want to be careful not to include JQuery twice! -->
    
     <!--  -->
      <script src="./JinlanFu/jquery-1.11.2.min.js"></script>
    
  
    <!-- doing something a bit funky here because I want to be careful not to include JQuery twice! -->
    
	<script src="./JinlanFu/bootstrap.min.js"></script>
    
  
    <!-- doing something a bit funky here because I want to be careful not to include JQuery twice! -->
    
	<script src="./JinlanFu/main.js"></script>
    
  



	<!-- Google Analytics -->
	<script>
		(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
		(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
		m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
		})(window,document,'script','//www.google-analytics.com/analytics.js','ga');
		ga('create', 'UA-74487606-1', 'auto');
		ga('send', 'pageview');
	</script>
	<!-- End Google Analytics -->


  
  

<!-- <iframe scrolling="no" frameborder="0" allowtransparency="true" src="./JinlanFu/widget_iframe.0504c5db6e58d499a7ba93c246a8554d.html" title="Twitter settings iframe" style="display: none;"></iframe><iframe id="rufous-sandbox" scrolling="no" frameborder="0" allowtransparency="true" allowfullscreen="true" style="position: absolute; visibility: hidden; display: none; width: 0px; height: 0px; padding: 0px; border: none;" title="Twitter analytics iframe" src="./JinlanFu/saved_resource(1).html"></iframe> -->

</body><grammarly-desktop-integration data-grammarly-shadow-root="true"></grammarly-desktop-integration></html>